{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a type of linear regression technique that adds a regularization term to the cost function. The regularization term is the sum of the absolute values of the model parameters (coefficients), which is multiplied by a hyperparameter (λ). The cost function for Lasso Regression is:\n",
    "\n",
    "$\\huge \\text{Cost Function} = \\text{RSS} + \\lambda \\sum_{i=1}^{n} | \\beta_i | $\n",
    "\n",
    "Where:\n",
    "- RSS is the Residual Sum of Squares (standard least-squares error),\n",
    "- $ \\beta_i $ are the coefficients of the model, \n",
    "- $ \\lambda $ is the regularization parameter that controls the amount of shrinkage applied to the coefficients.\n",
    "\n",
    "## Key Differences from Other Regression Techniques:\n",
    "\n",
    "### 1. Regularization:\n",
    "Lasso applies L1 regularization, which tends to shrink the coefficients of less important features to zero. This makes Lasso useful for feature selection, as it can effectively eliminate irrelevant features from the model.\n",
    "\n",
    "- **Ridge Regression** uses L2 regularization, where the penalty is the sum of squared coefficients. Unlike Lasso, Ridge does not shrink coefficients to zero but instead reduces their magnitude.\n",
    "- **Linear Regression** (without regularization) only minimizes the sum of squared residuals (RSS) and does not impose any penalty on the size of coefficients.\n",
    "\n",
    "### 2. Feature Selection:\n",
    "Lasso Regression can result in sparse models, meaning it can drive some feature coefficients to exactly zero, effectively selecting a subset of features. This is in contrast to Ridge, which does not eliminate features but only shrinks them.\n",
    "\n",
    "### 3. Multicollinearity:\n",
    "Lasso handles multicollinearity (when features are highly correlated) by selecting one of the correlated features and shrinking the coefficients of others to zero, thus reducing model complexity. Ridge, on the other hand, spreads the weights across correlated features.\n",
    "\n",
    "### Summary:\n",
    "- **Lasso Regression**: L1 regularization, performs feature selection, can shrink coefficients to zero.\n",
    "- **Ridge Regression**: L2 regularization, reduces the magnitude of coefficients but doesn't eliminate them.\n",
    "- **Linear Regression**: No regularization, fits the data but can overfit if the model has many features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "The main advantage of using **Lasso Regression** in feature selection is its ability to produce **sparse models** by shrinking less important feature coefficients to exactly **zero**. This makes Lasso an effective tool for **automatic feature selection**.\n",
    "\n",
    "## Key Advantages:\n",
    "1. **Simplicity**: Lasso simplifies the model by selecting only the most important features and ignoring the rest. This results in a more interpretable model.\n",
    "   \n",
    "2. **Dimensionality Reduction**: By shrinking some feature coefficients to zero, Lasso reduces the number of features considered in the final model, helping to combat the **curse of dimensionality** in datasets with many features.\n",
    "\n",
    "3. **Prevents Overfitting**: By reducing the number of non-zero features, Lasso helps prevent overfitting in high-dimensional datasets, leading to models that generalize better to unseen data.\n",
    "\n",
    "4. **Efficiency**: Lasso performs feature selection **during the model training process**, eliminating the need for a separate feature selection step, making it computationally efficient.\n",
    "\n",
    "### Summary:\n",
    "Lasso's ability to perform **automatic feature selection** by driving less important coefficients to zero is its key advantage, leading to simpler, more interpretable models that generalize well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "Interpreting the coefficients of a **Lasso Regression** model follows a similar approach to interpreting coefficients in standard linear regression, but with additional insights due to the regularization effect.\n",
    "\n",
    "### Key Points in Interpreting Lasso Coefficients:\n",
    "\n",
    "1. **Magnitude and Sign of Coefficients**:\n",
    "   - A positive coefficient ($ \\beta_i > 0 $) means that an increase in the corresponding feature will result in an increase in the predicted value.\n",
    "   - A negative coefficient ($ \\beta_i < 0 $) means that an increase in the corresponding feature will result in a decrease in the predicted value.\n",
    "   - The magnitude of the coefficient indicates the strength of the relationship between the feature and the target variable.\n",
    "\n",
    "2. **Coefficients Shrunk to Zero**:\n",
    "   - In Lasso Regression, some coefficients may be exactly zero. This indicates that the corresponding feature has been **excluded** from the model, meaning that Lasso has determined it to be **irrelevant** or unimportant for predicting the target variable.\n",
    "   - Features with zero coefficients can be safely ignored without impacting model performance.\n",
    "\n",
    "3. **Effect of Regularization (λ)**:\n",
    "   - The regularization parameter $ \\lambda $ controls the degree of shrinkage applied to the coefficients. A larger $ \\lambda $ increases the penalty, resulting in more coefficients being driven to zero, making the model more sparse.\n",
    "   - As $ \\lambda $ increases, Lasso emphasizes the most important features and discards those with less predictive power.\n",
    "\n",
    "4. **Comparison to Ordinary Least Squares (OLS)**:\n",
    "   - In standard linear regression (OLS), all features are retained, and their coefficients are estimated based on the fit to the data. Lasso, on the other hand, will shrink less important coefficients to zero, providing a more parsimonious model by retaining only the most influential features.\n",
    "\n",
    "### Example:\n",
    "For a Lasso Regression model with three features:\n",
    "- $ \\beta_1 = 0.5 $: Feature 1 positively affects the target, and an increase in Feature 1 increases the predicted value.\n",
    "- $ \\beta_2 = 0 $: Feature 2 has been excluded by Lasso, indicating it does not contribute to the prediction.\n",
    "- $ \\beta_3 = -0.3 $: Feature 3 negatively affects the target, and an increase in Feature 3 decreases the predicted value.\n",
    "\n",
    "### Summary:\n",
    "- Coefficients provide the same directional and magnitude interpretations as in linear regression.\n",
    "- Coefficients shrunk to zero indicate irrelevant features that Lasso has excluded from the model.\n",
    "- The regularization parameter $ \\lambda $ controls the degree of feature selection by shrinking less important features toward zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "\n",
    "The main tuning parameter in **Lasso Regression** is the **regularization strength** (denoted by $ \\lambda $ or sometimes $ \\alpha $), which controls the degree of shrinkage applied to the coefficients. This parameter directly affects the model’s complexity and performance.\n",
    "\n",
    "## Key Tuning Parameters in Lasso Regression:\n",
    "\n",
    "### 1. **Regularization Strength ($ \\lambda $)**:\n",
    "   - **Description**: $ \\lambda $ (or $ \\alpha $) is the most important tuning parameter in Lasso Regression. It controls the strength of the L1 regularization applied to the model coefficients.\n",
    "   - **Effect**:\n",
    "     - **High $ \\lambda $**: Stronger regularization is applied, which shrinks more coefficients towards zero. This leads to simpler, sparser models with fewer features but can increase bias, potentially leading to underfitting if $ \\lambda $ is too large.\n",
    "     - **Low $ \\lambda $**: Weaker regularization results in a model closer to standard linear regression, where most coefficients remain non-zero. A very small $ \\lambda $ can lead to overfitting as the model may capture noise in the training data.\n",
    "   - **Impact on Model Performance**:\n",
    "     - **High $ \\lambda $**: Fewer features, lower variance, more bias.\n",
    "     - **Low $ \\lambda $**: More features, higher variance, less bias.\n",
    "     - The ideal $ \\lambda $ balances bias and variance, yielding a model that generalizes well to unseen data.\n",
    "\n",
    "### 2. **Normalization / Scaling of Features**:\n",
    "   - **Description**: Lasso Regression can be sensitive to the scale of the input features. If the features have different scales (e.g., age in years vs. income in dollars), the regularization term can disproportionately affect the larger scale features.\n",
    "   - **Effect**:\n",
    "     - **Normalization** ensures that all features are on the same scale, making the regularization apply evenly across all coefficients.\n",
    "     - Without scaling, features with larger ranges may dominate and bias the model.\n",
    "   - **Impact on Model Performance**:\n",
    "     - Normalization leads to more accurate and fair shrinkage across all features, improving the model's feature selection process and overall performance.\n",
    "\n",
    "### 3. **Cross-Validation**:\n",
    "   - **Description**: Cross-validation is often used to find the optimal $ \\lambda $ by evaluating model performance across multiple splits of the dataset.\n",
    "   - **Effect**:\n",
    "     - Cross-validation ensures that the chosen $ \\lambda $ generalizes well to unseen data, preventing overfitting or underfitting.\n",
    "     - Commonly, **k-fold cross-validation** or **leave-one-out cross-validation (LOOCV)** is used to tune $ \\lambda $.\n",
    "   - **Impact on Model Performance**:\n",
    "     - Proper cross-validation helps in identifying the best regularization strength to optimize model performance on unseen data.\n",
    "\n",
    "### 4. **Maximum Number of Iterations**:\n",
    "   - **Description**: The maximum number of iterations controls how many times the optimization algorithm runs before it converges.\n",
    "   - **Effect**:\n",
    "     - If the optimization does not converge within the given iterations, the algorithm may return suboptimal results.\n",
    "   - **Impact on Model Performance**:\n",
    "     - Increasing the number of iterations can improve convergence, but too many iterations may increase computation time without significant benefits.\n",
    "\n",
    "### 5. **Tolerance for Optimization**:\n",
    "   - **Description**: Tolerance is the threshold that determines when the optimization algorithm should stop iterating. It defines how close the algorithm must get to the optimal solution before it stops.\n",
    "   - **Effect**:\n",
    "     - Lower tolerance leads to a more precise solution but may require more iterations.\n",
    "     - Higher tolerance can result in faster convergence but may not yield the best coefficients.\n",
    "   - **Impact on Model Performance**:\n",
    "     - Lower tolerance improves the accuracy of the solution but increases computation time.\n",
    "\n",
    "## Summary:\n",
    "- **$ \\lambda $** is the key tuning parameter in Lasso Regression, controlling the strength of regularization and affecting the balance between bias and variance.\n",
    "- **Normalization** of features is important to ensure the regularization applies evenly across features.\n",
    "- **Cross-validation** helps to find the optimal $ \\lambda $ for generalization.\n",
    "- **Maximum iterations** and **tolerance** affect the convergence and accuracy of the optimization process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\n",
    "**Lasso Regression** is fundamentally a linear regression technique, meaning it assumes a linear relationship between the input features and the target variable. However, it can still be used for **non-linear regression problems** by transforming the input features in a way that allows the model to capture non-linear relationships.\n",
    "\n",
    "## How to Use Lasso Regression for Non-Linear Problems:\n",
    "\n",
    "### 1. **Feature Engineering**:\n",
    "   - **Transform input features** to capture non-linear relationships. This can be done by creating new features that represent non-linear combinations of the original features. Common transformations include:\n",
    "     - **Polynomial features**: Add powers of the original features (e.g., $ x^2, x^3 $) to model polynomial relationships.\n",
    "     - **Interaction terms**: Include products of features (e.g., $ x_1 \\times x_2 $) to capture interactions between variables.\n",
    "\n",
    "   - By applying these transformations, the Lasso model is still technically linear in the **new transformed feature space** but can capture non-linear patterns in the original data.\n",
    "\n",
    "   - **Example**:\n",
    "     Suppose you have a non-linear relationship like:\n",
    "     $  y = 2x^2 + 3x + 1 $\n",
    "     You can create a new feature $ z = x^2 $, and apply Lasso Regression on both $ z $ and $ x $, which will allow the model to capture the non-linear relationship.\n",
    "\n",
    "### 2. **Kernel Trick** (Indirect Approach):\n",
    "   - Although Lasso Regression itself does not have a built-in kernel method (unlike Support Vector Machines or kernelized Ridge Regression), you can still approximate non-linearities by manually **applying kernel functions** to your data.\n",
    "   - For instance, apply **Gaussian kernels**, **polynomial kernels**, or **radial basis functions (RBF)** to map the original data into a higher-dimensional space where the relationship between features and the target is linear, then apply Lasso Regression in this new space.\n",
    "\n",
    "### 3. **Spline Regression**:\n",
    "   - Use **splines** (piecewise polynomials) to model non-linear relationships. Splines divide the data into segments and fit a different polynomial to each segment, allowing for flexibility in capturing non-linear trends.\n",
    "   - After transforming the data into splines, Lasso Regression can be applied to the spline-transformed features for regularization and feature selection.\n",
    "\n",
    "## Limitations:\n",
    "   - Lasso itself cannot capture non-linear patterns **directly**. It requires feature transformations or the addition of non-linear basis functions (e.g., polynomials, splines) to handle non-linearity.\n",
    "   - Lasso works best when combined with thoughtful feature engineering or data transformations tailored to the specific non-linearity in the problem.\n",
    "\n",
    "## Summary:\n",
    "Yes, Lasso Regression can be used for non-linear regression problems, but it requires **feature transformation** techniques, such as creating polynomial features, interaction terms, or splines, to capture the non-linear relationships in the data. The model remains linear in the transformed feature space but can approximate non-linear patterns in the original feature space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "Both **Ridge Regression** and **Lasso Regression** are types of linear regression models that use regularization to prevent overfitting. However, they differ in the type of regularization applied and how they handle feature selection and coefficient shrinkage.\n",
    "\n",
    "## Key Differences:\n",
    "\n",
    "### 1. **Type of Regularization**:\n",
    "   - **Ridge Regression**: Uses **L2 regularization**, which adds a penalty proportional to the **square of the coefficients** to the cost function.\n",
    "\n",
    "      - $\\huge \\text{Cost Function} = \\text{RSS} + \\lambda \\sum_{i=1}^{n} \\beta_i^2$\n",
    "   - **Lasso Regression**: Uses **L1 regularization**, which adds a penalty proportional to the **absolute value of the coefficients** to the cost function.\n",
    "\n",
    "      - $\\huge \\text{Cost Function} = \\text{RSS} + \\lambda \\sum_{i=1}^{n} | \\beta_i |$\n",
    "\n",
    "### 2. **Feature Selection**:\n",
    "   - **Ridge Regression**: Does **not perform feature selection**. It shrinks coefficients but rarely drives them to zero, meaning all features remain in the final model.\n",
    "   - **Lasso Regression**: Performs **automatic feature selection** by shrinking some coefficients to exactly **zero**, effectively removing irrelevant features from the model.\n",
    "\n",
    "### 3. **Handling of Coefficients**:\n",
    "   - **Ridge Regression**: Reduces the magnitude of all coefficients but generally does not eliminate any features. It distributes the shrinkage evenly across all coefficients.\n",
    "   - **Lasso Regression**: Can shrink some coefficients to zero, resulting in sparse models where only a subset of the features are retained.\n",
    "\n",
    "### 4. **Effect on Multicollinearity**:\n",
    "   - **Ridge Regression**: Works well in situations with **multicollinearity** (high correlation between features) by distributing the coefficients across correlated features.\n",
    "   - **Lasso Regression**: In the presence of multicollinearity, Lasso tends to select one feature from a group of correlated features and shrink the others to zero.\n",
    "\n",
    "### 5. **Bias-Variance Tradeoff**:\n",
    "   - **Ridge Regression**: Leads to **lower variance** in the model by shrinking coefficients but can introduce **higher bias**.\n",
    "   - **Lasso Regression**: Typically leads to **higher bias** but can result in **lower variance**, especially when many features are irrelevant and can be eliminated.\n",
    "\n",
    "### 6. **When to Use**:\n",
    "   - **Ridge Regression**: Preferred when **all features** are expected to have some influence on the target variable, and multicollinearity needs to be addressed without feature elimination.\n",
    "   - **Lasso Regression**: Preferred when **some features** are expected to be irrelevant or unimportant, and feature selection is desired.\n",
    "\n",
    "## Summary:\n",
    "\n",
    "| **Aspect**              | **Ridge Regression**                        | **Lasso Regression**                     |\n",
    "|-------------------------|---------------------------------------------|------------------------------------------|\n",
    "| **Regularization Type**  | L2 (sum of squared coefficients)            | L1 (sum of absolute coefficients)        |\n",
    "| **Feature Selection**    | No (keeps all features)                     | Yes (shrinks some coefficients to zero)  |\n",
    "| **Coefficient Shrinkage**| Shrinks but doesn't eliminate coefficients  | Can shrink coefficients to zero          |\n",
    "| **Multicollinearity**    | Spreads weights across correlated features  | Selects one feature, shrinks others to zero |\n",
    "| **Bias-Variance Tradeoff** | Lower variance, higher bias               | Higher bias, lower variance              |\n",
    "\n",
    "In essence, **Ridge Regression** is useful when all features are relevant, while **Lasso Regression** is ideal for automatic feature selection by eliminating irrelevant features from the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\n",
    "Yes, **Lasso Regression** can handle **multicollinearity** in the input features, but it does so in a unique way compared to other techniques like Ridge Regression. Multicollinearity occurs when two or more input features are highly correlated, leading to instability in the coefficient estimates in standard linear regression models.\n",
    "\n",
    "## How Lasso Regression Handles Multicollinearity:\n",
    "\n",
    "### 1. **Feature Selection through Coefficient Shrinkage**:\n",
    "   - **Lasso Regression** uses **L1 regularization**, which has the property of shrinking some coefficients to **exactly zero**. When features are highly correlated (multicollinear), Lasso tends to select only one feature from the correlated group and shrinks the coefficients of the others to zero.\n",
    "   - This helps in **reducing multicollinearity** by eliminating redundant features from the model, leaving only one representative feature for each correlated group.\n",
    "\n",
    "### 2. **Preference for Simplicity**:\n",
    "   - Since Lasso shrinks irrelevant or redundant features to zero, it inherently prefers a simpler model. In the presence of multicollinearity, Lasso will retain only the most important features, effectively resolving the issue of **instability** caused by correlated features.\n",
    "   - The remaining features after Lasso's shrinkage are less likely to be highly correlated, leading to more stable coefficient estimates.\n",
    "\n",
    "### 3. **Contrast with Ridge Regression**:\n",
    "   - Unlike **Ridge Regression**, which uses L2 regularization and spreads the weights across all correlated features (without eliminating any), Lasso simplifies the model by selecting only a few features. This is especially useful in datasets where you expect many features to be irrelevant or redundant.\n",
    "\n",
    "### 4. **Caveat**:\n",
    "   - While Lasso can handle multicollinearity, it may arbitrarily choose one feature from a set of correlated features and discard the rest, even if they are equally informative. This can be a limitation if interpretability or equal treatment of correlated variables is important.\n",
    "   \n",
    "### Example:\n",
    "- If two features $ x_1 $ and $ x_2 $ are highly correlated, Lasso will likely shrink the coefficient of one of them to zero while retaining the other. This reduces the variance introduced by multicollinearity and leads to a more stable model.\n",
    "\n",
    "## Summary:\n",
    "- **Yes**, Lasso Regression can handle multicollinearity by performing **automatic feature selection**. It tends to pick one feature from a group of correlated features and shrink the others to zero, reducing redundancy and improving model stability.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
